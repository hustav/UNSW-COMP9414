{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda72a13-e5e6-4224-9885-e3e99d95e73f",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hustav/UNSW-COMP9414/blob/main/Week07/COMP9414-Week07-Neural-Networks.ipynb)\n",
    "\n",
    "# Training and Assessing Neural Networks with Keras\n",
    "\n",
    "**COMP9414 W07 Tutorial**\n",
    "\n",
    "- Instructor: Gustavo Batista\n",
    "- School of Computer Science and Engineering, UNSW Sydney\n",
    "- Notebook designed by Gustavo Batista\n",
    "- Last Update 2nd October 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253e470a-52a9-4fca-b17a-fedd35efbcc5",
   "metadata": {},
   "source": [
    "In this week's tutorial, we will train and assess a multi-layer perceptron, a type of \"deep\" neural network architecture. We will start with the well-known benchmark dataset, MNIST, with images of single handwritten digits (0-9).\n",
    "\n",
    "We will use Keras as our main framework for implementing deep learning models. Keras is now part of the TensorFlow framework and provides a simple library for learning these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ae70bb-242e-4a62-b0de-c45c05743955",
   "metadata": {},
   "source": [
    "## Technical prerequisites\n",
    "\n",
    "You will need the following packages installed to run this notebook:\n",
    "\n",
    "1. Numpy\n",
    "2. Matplotlib\n",
    "3. Scikit-learn\n",
    "4. Tensorflow\n",
    "\n",
    "The first three libraries are often found in most installations. If they are not installed on your system, you can install them using the `pip` or `conda` commands.\n",
    "\n",
    "TensorFlow usually requires an older Python version. If you have installation conflicts, we suggest creating an environment for TensorFlow with a compatible Python version. Alternatively, you can run this notebook on Google Colab (see link in the first cell), which has all library requirements installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75e0c0-45f2-4aab-9ba0-fa2724c38d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9366a01-0b3c-4101-8174-f208a46b30f8",
   "metadata": {},
   "source": [
    "The following line will tell you how many CPUs and GPUs your system has. If you have a GPU, you can expect a speed-up in model training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c418a5e8-7ea8-49c0-bec2-88cd69cb9b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number CPUs Available: \", len(tf.config.list_physical_devices('CPU')))\n",
    "print(\"Number GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f4b9d3-d231-4917-ae52-1a0347966928",
   "metadata": {},
   "source": [
    "## The MNIST dataset\n",
    "\n",
    "We will train a simple, fully connected neural network using Keras to classify handwritten digits from the MNIST dataset. \n",
    "\n",
    "The MNIST dataset contains 70,000 grayscale images of handwritten digits (0-9), each of size 28x28 pixels. This dataset is very popular for benchmarking Machine Learning models. It is even available as part of the TensorFlow installation.\n",
    "\n",
    "We can use the following command to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f844e41-77e3-473d-b7b8-a614c4d04b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30efb0a-322a-47bc-b94a-d3ec59a14035",
   "metadata": {},
   "source": [
    "MNIST comes with a standard splitting of training and test sets. The training set should be used for model fitting and hyperparameter search, and the test set should be used exclusively for model evaluation. Using the test set for any other task is a methodological mistake. \n",
    "\n",
    "We have the following data splits:\n",
    "\n",
    "1. `X_train`: the training data we will use to fit our model parameters.\n",
    "2. `y_train`: the associated labels for each training case.\n",
    "3. `X_test`: the test data we will use to assess the model performance.\n",
    "4. `y_test`: the associated labels for each test instance.\n",
    "\n",
    "The MNIST is a relatively large dataset. It has 60,000 training images and 10,000 test images, as we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b8e89-9af1-46b4-aa12-5cb3da9e554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MNIST size (number of images, number of image lines, number of image columns)\")\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(\"These are the labels, each training or testing image has an associated label (0-9)\")\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32155cf8-3c0d-49ca-8fe5-6783ab1b3567",
   "metadata": {},
   "source": [
    "Let's visualise some digits to understand how complex this problem is. The following cell plots the first six digits in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774d7d0a-56fc-4efe-8fef-43118114c873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualise the first six digits in the training set\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(X_train[i], cmap='gray')\n",
    "    plt.title(f'Label: {y_train[i]}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5485cb31-b3f9-47c2-a776-e2bde0742883",
   "metadata": {},
   "source": [
    "We can also visualise some images from the same digit to get an idea of the variability in each class. Feel free to change the first line to see images of the other digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a3f9fd-cfa4-490d-b45c-b551a6e56c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the first six 'five' digits in the training set\n",
    "indices = np.where(y_train == 5)[0]  # Find the indices of the images for class 5\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(X_train[indices[i]], cmap='gray')\n",
    "    plt.title(f'Label: {y_train[indices[i]]}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c59ac9-8abd-4635-bdd4-87e792bf1c58",
   "metadata": {},
   "source": [
    "Before we conclude this section about the dataset, we will make two simple pre-processing:\n",
    "\n",
    "1. Each pixel is represented by a number between 0 and 255. We will normalize the pixels to be numbers between 0 and 1, which will facilitate training.\n",
    "2. We will convert the class numbers into a one-hot encoding. One-hot encodings represent the output as a vector. Each entry represents one class, and the vector will have zeros for all entries but one that represents the class.\n",
    "\n",
    "These are examples of one-hot encodings:\n",
    "1. The digit 0 is represented as [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "2. The digit 5 is represented as [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "3. The digit 9 is represented as [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab936c-085f-4222-aa07-60005dc08dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_one_hot = to_categorical(y_train, 10)\n",
    "y_test_one_hot = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95634aee-1201-433a-8576-6f8975b834f5",
   "metadata": {},
   "source": [
    "## Defining a model Neural Network model with Keras\n",
    "\n",
    "Keras makes it very simple to create a deep-learning model. The process has three main steps:\n",
    "\n",
    "1. Model definition: We declare the model architecture, specifying the properties of each layer, such as the number of units and activation function.\n",
    "2. Model compilation: This step prepares the model for training, specifying aspects such as the optimizer, the loss function, and the metrics to be monitored during training.\n",
    "3. Model fitting: This step fits the model to the dataset. In this step, we need to specify the training data, number of epochs, batch size, and validation data.\n",
    "\n",
    "Let's define our first model with the following architecture:\n",
    "\n",
    "1. 784 input units that correspond to the number of pixels in the image ($28 \\times 28$). In this case, each normalised pixel colour will be one input to the model.\n",
    "2. 64 hidden units. This design decision is difficult to justify, as the hidden units should be proportional to the data's \"complexity.\" We will play with this hyperparameter later.\n",
    "3. 10 output units. This is the number of classes, and each unit corresponds to one digit (class) in the one-hot encoding.\n",
    "\n",
    "The following figure illustrates the neural network architecture.\n",
    "\n",
    "![First architecture](img/first_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a14dee-0641-42d3-827b-751fbc339eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(28, 28)),                  # Input is a 28x28 image\n",
    "    Flatten(),                              # Flatten the 28x28 input images into a 784-length vector  \n",
    "    Dense(64, activation='relu'),           # Hidden layer with 64 neurons and ReLU activation \n",
    "    Dense(10, activation='softmax')         # Output layer with 10 neurons (one for each digit)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b55f0-7aca-4add-9c43-6cd3afb3b19a",
   "metadata": {},
   "source": [
    "There are several observations for the previous cell:\n",
    "\n",
    "1. The sequential model specifies that we are creating a neural network with a sequence of layers. It is a sequence in that the input layer feeds the hidden layer that feeds the output layer.\n",
    "2. The input is $28 \\times 28$ images, but the neural network has 784 input units. Therefore, the `Flatten` statement \"flats\" the matrix into a vector.\n",
    "3. We follow up with two additional dense layers. The first is a hidden layer with ReLU activation, and the second is the output layer with softmax activation.\n",
    "\n",
    "Now, we are ready to compile our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cc53e5-a5ad-4129-8f87-772d6deca063",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689bf73c-2c07-4d1e-a99e-2e12ac44a0b2",
   "metadata": {},
   "source": [
    "Notice that in this compilation step, we specified Adam as the optimiser, cross-entropy as the loss function, and accuracy as the performance measure for later model assessment.\n",
    "\n",
    "Finally, we can train the model for 10 epochs with a batch size of 32 images. This will take a while to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8164dcfd-1dfd-4863-b45e-b7e986012249",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train_one_hot, epochs=10, validation_split=0.2, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fbb9eb-fc71-4af4-afa2-6166d5f58937",
   "metadata": {},
   "source": [
    "We use 20% of the training set as a validation set. This validation set helps us to understand if the model is overfitting the training data. The following figure plots the training and validation loss and accuracy during the model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d0408-6449-476e-8c26-a8589d3d084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64cb8af-0a83-462a-bb8f-5e0323a58cea",
   "metadata": {},
   "source": [
    "Finally, we can assess the model's performance by measuring its accuracy in the test set. The test set is a partition of the data used **only** for model assessment. It should never be used for model fitting or hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b09e0-0e4d-4416-8f52-5b0ae0bb785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot)\n",
    "print(f'Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6cd16b-e656-4b80-9c6c-4de704c6a370",
   "metadata": {},
   "source": [
    "Nice! We created a model with 97.31% accuracy! This is pretty accurate and may indicate that the MNIST dataset is not very difficult. \n",
    "\n",
    "Our model makes less than 3% of mistakes in a 10,000-sample test set. Therefore, it misclassifies around 260 images. The classifier's accuracy/error gives us no information about whether we perform equally well in all classes or whether some are more difficult. \n",
    "\n",
    "In the next cell, we will exercise to plot the confusion matrix. This will tell us which classes are the most misclassified and how they are misclassified among them.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "A confusion matrix is a performance evaluation tool for classification models. It provides a visual summary of the model’s predictions compared to the actual values. The confusion matrix is a table. Each row represents the instances of an actual class, while each column represents the instances of a predicted class, making it easy to see which classes are most often confused with each other. A perfect model would have non-zero values only along the diagonal of the confusion matrix, indicating that all predictions are correct.\n",
    "\n",
    "The scikit-learn library has the [``confusion_matrix``](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and [``ConfusionMatrixDisplay``](https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) functions that compute and plot confusion matrices (see the links for the official documentation). \n",
    "\n",
    "We will use the method [``model.predict``](https://keras.io/api/models/model_training_apis/) to return the classification for the test cases. The predictions come in as one-hot encoding, and we will need to convert them to numbers between 0 and 9 using the [``np.argmax``](https://numpy.org/doc/2.0/reference/generated/numpy.argmax.html) method. \n",
    "\n",
    "The next cell will compute and plot the confusion matrix. We have done most of the work for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8fb794-8497-4bfe-872f-02e755ec0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = ...                                         # TODO, call model.predict to predict the labels for X_test\n",
    "y_pred_classes = ...                                 # TODO, call np.argmax to transform the predicted class probabilities (y_pred) into class 0-9 predictions\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = ...                                             # TODO, call confusion_matrix and inform the actual labels (y_test) and the predicted labels (y_pred_classes)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(10))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f2dd23-997e-46cf-96a6-966c7364a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(10))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844f7ef7-0505-4b33-acc8-a91b6755619b",
   "metadata": {},
   "source": [
    "## Qualitative model assessment\n",
    "\n",
    "So far, we have assessed our model quantitatively. In the next part, we will conduct a more qualitative assessment. In a qualitative assessment, we examine the misclassified images. We aim to understand if the model is making silly mistakes or if the data has difficult images. For instance, the confusion matrix has shown us that the digit \"9\" is often misclassified, particularly with the digit 4. The qualitative analysis will help us to understand if these two digits are truly similar or if the model still has room for improvement.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Let's plot 100 incorrect predictions to better understand the classifier's performance. We have done most of the heavy lifting for you; you just need to complete one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56084707-f2c1-4517-8709-1a3791a839a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified images\n",
    "misclassified_indices = ...                                         # TODO. Use np.where to find the indices of the images where y_pred_classes differs from y_test\n",
    "\n",
    "# Plot a 10x10 grid of misclassified images\n",
    "num_images = 100  # Number of misclassified images to display\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i, index in enumerate(misclassified_indices[:num_images]):\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(X_test[index], cmap='gray')\n",
    "    plt.title(f\"True: {y_test[index]}\\nPred: {y_pred_classes[index]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a320ec-0a0f-49dd-9cba-5cf46c05956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified images\n",
    "misclassified_indices = np.where(y_pred_classes != y_test)[0]\n",
    "\n",
    "# Plot a 10x10 grid of misclassified images\n",
    "num_images = 100  # Number of misclassified images to display\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i, index in enumerate(misclassified_indices[:num_images]):\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(X_test[index], cmap='gray')\n",
    "    plt.title(f\"True: {y_test[index]}\\nPred: {y_pred_classes[index]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940051a3-1e2f-400e-b81b-7ddcdba779fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc5498ab-2300-420b-be5b-3e6970808b25",
   "metadata": {},
   "source": [
    "## Hyperparameter search\n",
    "\n",
    "Now that you understand how to create and assess your models let's see if we can improve the deep learning models' performance for the MNIST dataset.\n",
    "\n",
    "Deep learning models have several hyperparameters that can influence their performance. Here are some ideas:\n",
    "\n",
    "1. Model architecture: We could make the model architecture smaller or bigger and see if it can improve performance.\n",
    "2. We can train the models for a longer period.\n",
    "3. We can use a different optimiser, SGD, instead of ADAM.\n",
    "\n",
    "Unfortunately, testing all possible combinations of hyperparameters is very time-consuming. However, we can assess some of these ideas against our initial model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa732b8-9700-4f90-aa9f-f9d75826b45d",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Define a new architecture with 128 hidden units instead of 64. Does this change improve accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7789b17-9ced-4cb6-9507-549eb69fd36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "...                            # TODO\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_one_hot, epochs=10, validation_split=0.2, batch_size=32)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot)\n",
    "print(f'Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2274a237-2ae9-4c2d-8b90-188ecbdc8a0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(28, 28)),                  # Input is a 28x28 image\n",
    "    Flatten(),                              # Flatten the 28x28 input images into a 784-length vector  \n",
    "    Dense(128, activation='relu'),          # Hidden layer with 128 neurons and ReLU activation \n",
    "    Dense(10, activation='softmax')         # Output layer with 10 neurons (one for each digit)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_one_hot, epochs=10, validation_split=0.2, batch_size=32)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot)\n",
    "print(f'Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5094931-c4c1-4c9b-8c5f-2a6dd6520f1d",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Define a new architecture with no hidden layer. Does this change improve accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80ecbe1-b752-4a8c-a826-916c0f4c3063",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "...                            # TODO\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_one_hot, epochs=10, validation_split=0.2, batch_size=32)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot)\n",
    "print(f'Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb90387c-9a6b-4d72-92a9-c5ceeef46a30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(28, 28)),                  # Input is a 28x28 image\n",
    "    Flatten(),                              # Flatten the 28x28 input images into a 784-length vector  \n",
    "    Dense(10, activation='softmax')         # Output layer with 10 neurons (one for each digit)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_one_hot, epochs=10, validation_split=0.2, batch_size=32)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot)\n",
    "print(f'Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eae7c2a-10c4-40df-aa72-5f3651d2e1e5",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Train the original model for 30 epochs. Does this change improve accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb26b7e7-5be2-42c7-8b15-8fef4b1ac89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(28, 28)),                  # Input is a 28x28 image\n",
    "    Flatten(),                              # Flatten the 28x28 input images into a 784-length vector  \n",
    "    Dense(64, activation='relu'),           # Hidden layer with 64 neurons and ReLU activation \n",
    "    Dense(10, activation='softmax')         # Output layer with 10 neurons (one for each digit)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(...)                              # TODO\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot)\n",
    "print(f'Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa698bb3-e250-461a-afd8-9c95268e9518",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(28, 28)),                  # Input is a 28x28 image\n",
    "    Flatten(),                              # Flatten the 28x28 input images into a 784-length vector  \n",
    "    Dense(64, activation='relu'),           # Hidden layer with 64 neurons and ReLU activation \n",
    "    Dense(10, activation='softmax')         # Output layer with 10 neurons (one for each digit)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_one_hot, epochs=30, validation_split=0.2, batch_size=32)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot)\n",
    "print(f'Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c31625-01a6-4549-a130-9bb59fb07f3c",
   "metadata": {},
   "source": [
    "Well, it seems that only increasing the model capacity improved the accuracy by a small fraction. You can try other ideas, but MNIST is a relatively easy dataset and it becomes hard to make improvements when we are close to 100% accuracy.\n",
    "\n",
    "A more challenging dataset is the Fashion MNIST. It is a direct replacement for MNIST as it also consists of $28 \\times 28$-pixel images. However, the task is more complex as images have different types of clothing, and the shapes of the clothing items can overlap. Some items like “T-shirt” and “Pullover” are visually more similar compared to digits, making classification harder. Consequently, achieving more than 90% accuracy is more challenging.\n",
    "\n",
    "You can load the Fashion MNIST dataset with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010b5a0e-5f7e-4ef7-9976-2607c808ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalise the dataset (scale values to range 0-1)\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5f1d7d-7cd5-49ac-8d5d-72a30c30f6b1",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Propose a model for the Fashion MNIST dataset. Tune the hyperparameter and try to achieve a test set accuracy close to 90%. Our best models required a much larger capacity than our MNIST models, including more hidden layers and neurons per layer.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "Today, we have learned how to implement and evaluate deep learning models using Keras. In the next tutorial, we will look at decision trees and ensembles of these models, known as random forests. These are powerful models to learn from tabular datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
